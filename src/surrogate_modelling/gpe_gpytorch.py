"""
GPyTorch library for GP training
"""
import numpy as np
import sys
import sklearn
import scipy
import copy
from joblib import Parallel, delayed
import torch
import gpytorch
from scipy.optimize import dual_annealing, differential_evolution
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from gpytorch.kernels import RBFKernel, MaternKernel, ScaleKernel, SpectralMixtureKernel, GridInterpolationKernel


class MyExactGPyModel(gpytorch.models.ExactGP):
    """
    Instance of GPyTorch's "ExactGP" library, with custom likelihood, kernel, training points.

    The likelihood is kept constant: Gaussian Likelihood (https://docs.gpytorch.ai/en/latest/likelihoods.html)

    Parameters:
        :param train_x: <np.array[n_tp, n_p]> with parameter sets used to train GPR
        :param train_y: <np.array[n_tp, n_obs]> with forward model outputs used to trian GPR
        :param kernel: <kernel instance> with kernel used in GPR
        :param likelihood <likelihood instance> to train noise in GPR
    """

    def __init__(self, train_x, train_y, kernel, likelihood):
        super(MyExactGPyModel, self).__init__(train_inputs=train_x, train_targets=train_y, likelihood=likelihood)
        # self.mean_module = gpytorch.means.ConstantMean()
        self.mean_module = gpytorch.means.ZeroMean()
        self.covar_module = kernel

    def forward(self, x):
        """
        Takes in the training data (x) and returns a multivariate normal distribution with mean and covariance (kernel)
        set in "__init__()"
        :param x: training data (parameter sets)
        :return:
        """
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)


class GPyTraining:
    """
    Uses the 'GPyTorch' ExactGPR library to generate a GPE for a given forward model, based on collocation points
    generated by said forward model.

    Parameters:
        :param train_X = np.array(n_tp, n_p),
            with training points (parameter sets)
        :param train_Y = np.array(nn_tp, n_obs),
            model outputs in each location where the fcm was evaluated
        :param kernel_type = <str>
            with the type of kernel to use in the GPR. Default is 'RBF'
            1. 'RBF' - Radial Basis Function (Gaussion kernel)
            2. 'Matern' - Matern kernel
        :param kernel_isotropy = <bool>
            True to use isotropic kernel, False to use anisotropic kernel
        :param nugget = <float>
            with the nugget value to use in the GPR training
        :param n_restarts = <int>
            with the number of restarts to use in the optimization of the GPR hyperparameters
        :param noise = <bool>
            True to use noise in the GPR training, False to train without noise
        :param training_iter = <int>
            with the number of optimization iterations to train the GPR
        :param optimizer = <str>
            with the name of the optimizer to use in the GPR training. Default is 'Adam'
            1. 'Adam' - Adam optimizer
            2. 'LBFGS' - L-BFGS optimizer
        :param lr = <float>
            with the learning rate to use in the optimizer. Default is 0.5
        :param y_normalization: <bool> 
            True to normalize model outputs before training, False to train as is
        :param tp_normalization: bool, 
            False to use training points as they are, True to normalize TP. Default is True
        :param loss: <str>
            with the name of the loss function to use in the GPR training. Default is 'exact'
            1. 'exact' - Exact Marginal Log Likelihood
            2. 'loo' - Leave One Out Pseudo Likelihood
        :param weight_decay: <float>
            with the weight decay to use in the optimizer. Default is 0
        :param gradient_free_start: <bool>
            True to use a gradient-free optimizer to start the optimization, False to start with the optimizer
        :param parallelize : bool
            True to parallelize surrogate training, False to train in a sequential loop

    TODO: For GPyTorch, check the GPU settings (if needed) and other gpytorch.settings to predict values.
    """

    def __init__(self, train_X, train_y, 
                 kernel_type='RBF', kernel_isotropy=True, 
                 nugget=1e-6, n_restarts=1, noise=False,
                 training_iter=100, optimizer="adam", lr=0.5,
                 y_normalization=False, tp_normalization=True,
                 loss='exact', weight_decay=0,
                 gradient_free_start=False,
                 verbose=True,
                 parallelize=False):

        # Basic attributed
        self.X_train = train_X
        self.y_train = train_y

        # self.n_obs = self.model_evaluations.shape[1]
        self.ndim = train_X.shape[1]

        # GPyTorch inputs: 
        self.optimizer_ = optimizer
        self.training_iter = training_iter
        self.loss = loss
        self.n_restarts = n_restarts
        self.gradient_free_start = gradient_free_start
        self.lr = lr
        self.weight_decay = weight_decay

        # Kernel information:
        self.kernel_type = kernel_type
        self.kernel_isotropy = kernel_isotropy
        self.noise = noise

        self.parallel = parallelize
        self.verbose = verbose

        # Options for GPR library:
        self.tp_norm = tp_normalization
        self.y_norm = y_normalization

        self.gp_dict = {}
        self.hp_dict = {}
        self.loss_dict = {}
        self.y_scaler = {}

        # self._id_vectors(likelihood, kernel)

    @staticmethod
    def convert_to_tensor(array):
        """
        Function to transform np.array to a tensor
        Args:
            array: <np.array> that you want to change to a tensor

        Returns: <tensor> data in np.array transformed to tensor format
        """
        transformed = torch.tensor(array).float()
        return transformed

    def normalize_data(self, train_y):
        """
        Function to normalize training points outputs before training
        Args:
            train_y: <np.array[tp_size, n_obs]> with model output values to normalize

        Returns: <tensor> with normalized input values.
        """
        norm_y = (train_y - np.mean(train_y))/(np.std(train_y))
        train_y = self.convert_to_tensor(norm_y)
        return train_y

    def train_(self):
        """
        Function trains the surrogate model using the GPyTorch library, using the given optimizer.

        Returns:
        """
        # Transform the inputs
        if self.tp_norm:
            scaler_x_train = MinMaxScaler()
            scaler_x_train.fit(self.X_train)

            X_scaled = scaler_x_train.transform(self.X_train)
            self.x_scaler = scaler_x_train
        else:
            X_scaled = self.X_train
            self.x_scaler = None

        # Convert training points and prior parameter sets to tensor:
        tensor_x = self.convert_to_tensor(X_scaled)

        # Loop through output types:
        items = self.y_train.items()

        for key, output in items:
            n_obs = output.shape[1]

            if self.parallel and self.n_obs > 1:
                out_list = Parallel(n_jobs=-1, backend='multiprocessing')(delayed(self._fit)(train_x=tensor_x,
                                                                                             model_y=output[:, i])
                                                                     for i in range(n_obs))
            else:
                out_list = []
                for i, y_model in enumerate(output.T):
                    out = self._fit(train_x=tensor_x, model_y=y_model)

                    out_list.append(out)

            self.gp_dict[key] = {}
            self.hp_dict[key] = {}
            self.y_scaler[key] = {}
            # self.gp_score[key] = {}
            for i in range(n_obs):
                self.gp_dict[key][f'y_{i}'] = out_list[i]['gp']
                # self.gp_score[key][f'y_{i}'] = out_list[i]['R2']
                self.hp_dict[key][f'y_{i}'] = out_list[i]['hyper-parameters']
                self.y_scaler[key][f'y_{i}'] = out_list[i]['y_scaler']

            stop = 1

    @staticmethod
    def init_model_params(model):
        """
        Function to initialize model hyperparameters, for multi-start optimizations
        Args:
            model: GPyTorch instance
        Returns:
        
        """
        def initialize_tensor(param_):
            if len(param_.shape) < 2:
                # If the tensor has fewer than two dimensions, apply a different initialization method
                torch.nn.init.uniform_(param_)  # Example: Uniform initialization for tensors with fewer than 2 dimensions
            else:
                torch.nn.init.xavier_uniform_(param_)

        for name, param in model.named_parameters():
            initialize_tensor(param)

    def build_kernel(self):
        """Function initializes kernels for the GPR training
        ToDo: Add more kernels to the list, including spectral mixture, grid interpolation, etc.

        Returns: kernel, likelihood objects for GP training
        """

        # Initialize kernel
        ard_num_dims = self.ndim if not self.kernel_isotropy else None

        if 'rbf' in self.kernel_type.lower():
            kernel = RBFKernel(ard_num_dims=ard_num_dims)
        elif 'matern' in self.kernel_type.lower():
            kernel = MaternKernel(ard_num_dims=ard_num_dims)
        # elif 'spectral' in self.kernel_type.lower():
        #     kernel = SpectralMixtureKernel(num_mixtures=4)
        # elif 'grid' in self.kernel_type.lower():
        #     kernel = GridInterpolationKernel(RBFKernel(), grid_size=100, grid_bounds=[(0, 1)])
        else:
            print(f'Kernel type {self.kernel_type} not available. Defaulting to RBF kernel.')
            kernel = RBFKernel(ard_num_dims=ard_num_dims)

        kernel = ScaleKernel(kernel)

        # Initialize Likelihood
        if self.noise: 
            likelihood = gpytorch.likelihoods.GaussianLikelihood()
        else:
            ns = torch.ones(1) * 1e-7
            likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise=ns,
                                                                           noise_constraint=gpytorch.constraints.GreaterThan(1e-10))

        return kernel, likelihood
    
    def _fit_adam(self, train_x, model_y):
        """
        Function trains the GPR for a given training location, using the Adam optimizer"""
        # 0. Normalize, if needed, and transform model_evaluations at loc "i" to a tensor
        if self.y_norm:
            y_scaler = StandardScaler()
            y_scaler.fit(model_y.reshape(-1, 1))

            train_y = self.convert_to_tensor(y_scaler.transform(model_y.reshape(-1, 1)))
            train_y = train_y.squeeze()
            # train_y = self.normalize_tp(model_y)
        else:
            train_y = self.convert_to_tensor(model_y)

        best_loss = float('inf')
        best_params = None
        best_loss_list = None

        for i in range(self.n_restarts):
            
            # 1.Initialize kernel and likelihood:
            kernel, likelihood = self.build_kernel()

            # 2.Initialize instance of GPyTorch GPR:
            gp = MyExactGPyModel(train_x, train_y, kernel, likelihood)

            # Start training
            gp.train()
            likelihood.train()

            if i > 0:
                self.init_model_params(gp)

            # 3. Setup Optimizer
            optimizer = torch.optim.Adam(gp.parameters(), lr=self.lr, weight_decay=self.weight_decay)
            # elif self.optimizer_ == 'lbfgs':
                # optimizer = torch.optim.lbfgs(gp.parameters(), lr=self.lr)

            # Loss for GPs - log likelihood
            if 'exact' in self.loss.lower():
                mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp)
            elif 'loo' in self.loss.lower():
                mll = gpytorch.mlls.LeaveOneOutPseudoLikelihood(likelihood, gp)
            else: 
                sys.exit(f'Loss function {self.loss} not available.')

            # Change learning rate:
            scheduler = torch.optim.lr_scheduler.StepLR(optimizer,
                                                        step_size=50,
                                                        gamma=0.5)
            
            # if i == 0 and self.gradient_free_start:
            #     def negative_log_likelihood(params):
            #         """ Gradient-free optimizer from SciPy"""
            #         # Set the model hyperparameters based on the optimization parameters
            #         gp.covar_module.base_kernel.lengthscale = torch.tensor(params[0:self.n_params])
            #         gp.covar_module.outputscale = torch.tensor(params[-2])
            #         gp.likelihood.noise = torch.tensor(params[-1])

            #         # Zero out the gradients
            #         gp.zero_grad()

            #         # Forward pass to compute the negative log likelihood
            #         output = gp(train_x)
            #         loss = -mll(output, train_y)

            #         if self.verbose:
            #             print(f'Gradient-free start - Loss: {loss.item()}',
            #                   f'   Outputscale: {gp.covar_module.outputscale.item()}',
            #                   f'   lengthscale: {gp.covar_module.base_kernel.lengthscale[0]}',
            #                   f'   noise: {gp.likelihood.noise.item()}')

            #         # Return the negative log likelihood as a NumPy array
            #         return loss.item()

            #     # Define the bounds for the optimization parameters (lengthscale and outputscale)
            #     value = np.empty((), dtype=object)
            #     value[()] = (1e-3, 1e2)
            #     bounds_1 = list(np.full(self.n_params, value, dtype=object))
            #     bounds_2 = [(1e-2, 1e1), (1e-6, 1)]
            #     bounds = bounds_1 + bounds_2

            #     # Perform the global optimization using differential evolution
            #     # t1 = time.time()
            #     result = differential_evolution(negative_log_likelihood, bounds, maxiter=10)

            # Optimize parameters:
            loss_list = []
            for j in range(self.training_iter):
                # a. Zero gradients from previous iteration
                optimizer.zero_grad()
                # b. Output from model
                output = gp(train_x)
                # c. Calculate loss and back propagation gradients
                loss = -mll(output, train_y)
                loss.backward()
                optimizer.step()
                scheduler.step()   # Change learning rate
                loss_list.append(loss.item())   # Save loss, to plot later
                if self.verbose:
                    print(f'Iter {j + 1}/{self.training_iter} - Loss: {loss.item()}',
                          f'   Outputscale: {gp.covar_module.outputscale.item()}',
                          f'   lengthscale: {gp.covar_module.base_kernel.lengthscale[0]}',
                          f'   noise: {gp.likelihood.noise.item()}')
            
            if loss < best_loss:
                best_loss = loss
                best_params = gp.state_dict()
                best_loss_list = loss_list

        gp.load_state_dict(best_params)

        return gp, best_loss_list, y_scaler

    def _fit_lbfgs(self, train_x, model_y):
        """
        Function trains the GPR for a given training location, using the LBFGS optimizer
        ToDo: Check if the optimizer is working correctly, there are some convergence issues	
        """
        # 0. Normalize, if needed, and transform model_evaluations at loc "i" to a tensor
        if self.y_norm:
            y_scaler = StandardScaler()
            y_scaler.fit(model_y.reshape(-1, 1))

            train_y = self.convert_to_tensor(y_scaler.transform(model_y.reshape(-1, 1)))
            train_y = train_y.squeeze()
            # train_y = self.normalize_tp(model_y)
        else:
            train_y = self.convert_to_tensor(model_y)
            y_scaler = None

        best_loss = float('inf')
        best_params = None
        best_loss_list = None

        for i in range(self.n_restarts):

            # 1.Initialize kernel and likelihood:
            kernel, likelihood = self.build_kernel()

            # 2.Initialize instance of GPyTorch GPR:
            gp = MyExactGPyModel(train_x, train_y, kernel, likelihood)

            gp.covar_module.outputscale = torch.tensor(1.0)
            gp.covar_module.base_kernel.lengthscale = torch.tensor([1.0])
            likelihood.noise = torch.tensor(1e-2)  # Adjust noise level

            # Start training
            gp.train()
            likelihood.train()

            if i > 0:
                self.init_model_params(gp)

            # 3. Setup Optimizer
            optimizer = torch.optim.LBFGS(gp.parameters(), lr=self.lr)

            # Loss for GPs - log likelihood
            if 'exact' in self.loss.lower():
                mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp)
            elif 'loo' in self.loss.lower():
                mll = gpytorch.mlls.LeaveOneOutPseudoLikelihood(likelihood, gp)
            else:
                sys.exit(f'Loss function {self.loss} not available.')

            # Change learning rate:
            scheduler = torch.optim.lr_scheduler.StepLR(optimizer,
                                                        step_size=50,
                                                        gamma=0.5)
            
            # if i == 0 and self.gradient_free_start:
            #     def negative_log_likelihood(params):
            #         """ Gradient-free optimizer from SciPy"""
            #         # Set the model hyperparameters based on the optimization parameters
            #         gp.covar_module.base_kernel.lengthscale = torch.tensor(params[0:self.n_params])
            #         gp.covar_module.outputscale = torch.tensor(params[-2])
            #         gp.likelihood.noise = torch.tensor(params[-1])

            #         # Zero out the gradients
            #         gp.zero_grad()

            #         # Forward pass to compute the negative log likelihood
            #         output = gp(train_x)
            #         loss = -mll(output, train_y)

            #         if self.verbose:
            #             print(f'Gradient-free start - Loss: {loss.item()}',
            #                   f'   Outputscale: {gp.covar_module.outputscale.item()}',
            #                   f'   lengthscale: {gp.covar_module.base_kernel.lengthscale[0]}',
            #                   f'   noise: {gp.likelihood.noise.item()}')

            #         # Return the negative log likelihood as a NumPy array
            #         return loss.item()

            #     # Define the bounds for the optimization parameters (lengthscale and outputscale)
            #     value = np.empty((), dtype=object)
            #     value[()] = (1e-3, 1e2)
            #     bounds_1 = list(np.full(self.n_params, value, dtype=object))
            #     bounds_2 = [(1e-2, 1e1), (1e-6, 1)]
            #     bounds = bounds_1 + bounds_2

            #     # Perform the global optimization using differential evolution
            #     # t1 = time.time()
            #     result = differential_evolution(negative_log_likelihood, bounds, maxiter=10)

            # Optimize parameters:
            loss_list = []

            def closure():
                optimizer.zero_grad()
                output = gp(train_x)
                loss = -mll(output, train_y)
                # loss.backward()
                return loss

            loss = closure()
            loss.backward()
            for j in range(self.training_iter):
                # optimizer.step(closure)
                # loss = closure()
                # loss_list.append(loss.item())

                with gpytorch.settings.cholesky_jitter(1e-3):
                    loss = optimizer.step(closure)

                # loss = optimizer.step(closure)
                scheduler.step()  # Adjust the learning rate
                loss_list.append(loss.item())

                if self.verbose:
                    print(f'Iter {j + 1}/{self.training_iter} - Loss: {loss.item()}',
                          f'   Outputscale: {gp.covar_module.outputscale.item()}',
                          f'   lengthscale: {gp.covar_module.base_kernel.lengthscale[0]}',
                          f'   noise: {gp.likelihood.noise.item()}')

            if loss < best_loss:
                best_loss = loss
                best_params = gp.state_dict()
                best_loss_list = loss_list

        gp.load_state_dict(best_params)

        return gp, best_loss_list, y_scaler
            
    def _fit(self, train_x, model_y): 
        if self.optimizer_.lower() == 'adam':
            gp, loss_list, y_scaler = self._fit_adam(train_x, model_y)
        elif self.optimizer_.lower() == 'lbfgs':
            gp, loss_list, y_scaler = self._fit_lbfgs(train_x, model_y)
        else: 
            sys.exit(f'Optimizer {self.optimizer_} not available.')

        with torch.no_grad():
            return_out_dic = dict()
            return_out_dic['gp'] = gp
            return_out_dic['y_scaler'] = y_scaler
            # return_out_dic['likelihood'] = likelihood
            return_out_dic['loss'] = loss_list

            # Hyperparameters
            hp_dict = {'c_hp': gp.covar_module.outputscale.item(),
                       'cl_hp': gp.covar_module.base_kernel.lengthscale.numpy()[0, :]}
            try:
                hp_dict['noise_hp'] = gp.likelihood.noise.item()
            except:
                pass

            return_out_dic['hyper-parameters'] = hp_dict

            if self.y_norm:
                return_out_dic['y_norm'] = [np.mean(model_y), np.std(model_y)]

        return return_out_dic

    def predict_(self, x_):
        """
        Evaluates the surrogate models (for each loc) for each input parameter set, for each output type
        Args:
            x_: array[MC, n_params]
                with parameter sets to evaluate the surrogate models in


        Returns: dict
            with surrogate model mean (output) and the standard deviation (std) for each loc, size [n_obs, MC]
        """

        if self.tp_norm:
            x_scaled = self.x_scaler.transform(x_)
        else:
            x_scaled = x_

        prior_x = self.convert_to_tensor(x_scaled)

        items = self.gp_dict.items()

        # Loop over output types:
        y_pred = {}
        y_std = {}

        for key, gp_list in items:
            n_obs = len(gp_list)

            surrogate_prediction = np.zeros((x_.shape[0], n_obs))  # GPE mean, for each obs
            surrogate_std = np.zeros((x_.shape[0], n_obs))  # GPE mean, for each obs

            for i in range(n_obs):
                gp = gp_list[f'y_{i}']
                likelihood_ =gp.likelihood

                # Go into eval mode:
                gp.eval()
                likelihood_.eval()

                with torch.no_grad():
                    f_pred = likelihood_(gp(prior_x))
                    prediction = f_pred.mean.numpy()
                    std = f_pred.stddev.numpy()
                    if self.y_norm:  # Back-transform
                        # normalized results
                        pass
                        # prediction = self.gp_list[i]['y_norm'][1] * prediction + self.gp_list[i]['y_norm'][0]
                        # std = std * self.gp_list[i]['y_norm'][1]

                    if self.y_scaler[key][f'y_{i}'] is not None:
                        scaler = self.y_scaler[key][f'y_{i}']
                        prediction = prediction * scaler.scale_ + scaler.mean_
                        std = std * scaler.scale_

                    surrogate_prediction[:, i] = prediction
                    surrogate_std[:, i] = std

            y_pred[key] = surrogate_prediction
            y_std[key] = surrogate_std

        return y_pred, y_std

    @staticmethod
    def validation_error(true_y, sim_y, sim_std=None, std_metrics=False):
        """
        Estimates different evaluation (validation) criteria for a surrogate model, for each output location. Results for
        each output type are saved under different keys in a dictionary.
        Args:
            true_y: array [mc_valid, n_obs]
                simulator outputs for valid_samples
            sim_y: dict, with an array [mc_valid, n_obs] for each output type.
                surrogate/emulator's outputs for valid_samples.
            sim_std: dict, with an array [mc_valid, n_obs] for each output type.
                Surrogate/emulator standar deviation
            std_metrics: bool
                True to estimate error-based validation criteria. Default is False

        Returns: dict
            with validation criteria, a key for each criteria, and a subkey for each output type

        ToDo: add as part of MyGeneralGPR class, and the outputs are a dictionary, with output type as a key.
        """
        criteria_dict = {'rmse': dict(),
                         'mse': dict(),
                         'nse': dict(),
                         'r2': dict(),
                         'mean_error': dict(),
                         'std_error': dict()}

        if std_metrics:
            criteria_dict['norm_error'] = dict()
            criteria_dict['P95'] = dict()
            criteria_dict['DS'] = dict()

        for i, key in enumerate(sim_y):
            sm_out = sim_y[key]
            sm_std = sim_std[key]

            # RMSE
            criteria_dict['rmse'][key] = sklearn.metrics.mean_squared_error(y_true=true_y[key],
                                                                            y_pred=sm_out,
                                                                            multioutput='raw_values', squared=False)
            # MSE
            criteria_dict['mse'][key] = sklearn.metrics.mean_squared_error(y_true=true_y[key],
                                                                           y_pred=sm_out,
                                                                           multioutput='raw_values', squared=True)
            # NSE
            criteria_dict['nse'][key] = sklearn.metrics.r2_score(y_true=true_y[key],
                                                                 y_pred=sm_out,
                                                                 multioutput='raw_values')
            # Mean errors
            criteria_dict['mean_error'][key] = np.abs(
                np.mean(true_y[key], axis=0) - np.mean(sm_out, axis=0)) / np.mean(true_y[key], axis=0)

            criteria_dict['std_error'][key] = np.abs(
                np.std(true_y[key], axis=0) - np.std(sm_out, axis=0)) / np.std(true_y[key], axis=0)

            # R2 correlation
            criteria_dict['r2'][key] = np.zeros(sm_out.shape[1])
            for j in range(sm_out.shape[1]):
                criteria_dict['r2'][key][j] = np.corrcoef(true_y[key][:, j], sm_out[:, j])[0, 1]

            # Norm error
            if std_metrics and sim_std is not None:
                upper_ci = sm_out[:, i] + (1.96 * sm_std[:, i])
                lower_ci = sm_out[:, i] - (1.96 * sm_std[:, i])

                # Normalized error
                ind_val = np.divide(np.subtract(sm_out, true_y[key]), sm_std[key])
                criteria_dict['norm_error'][key] = np.mean(ind_val ** 2, axis=0)

                # P95
                p95 = np.where((true_y[key] <= upper_ci) & (
                        true_y[key] >= lower_ci), 1, 0)
                criteria_dict['P95'][key] = np.mean(p95, axis=0)

                # Dawid Score (https://www.jstor.org/stable/120118)
                criteria_dict['DS'][key] = np.mean(
                    ((np.subtract(sm_out, true_y[key])) / (sm_std ** 2)) + np.log(sm_std ** 2), axis=0)

        return criteria_dict
   

def save_valid_criteria(new_dict, old_dict, n_tp):
    """
    Saves the validation criteria for the current iteration (n_tp) to an existing dictionary, so we can have the
    results for all iterations in the same file. Each dictionary has a dictionary for each validation criteria.
    Each validation criteria has a key for each output type, which corresponds to a vector with n_loc, one value for
    each output value.
    Args:
        new_dict: Dict
            with the validation criteria for the current iteration
        old_dict: Dict
            With the validation criteria for all the previous iterations, including a key for N_tp, which saves
            the number of iteration.
        n_tp: int
            number of training points for the current BAL iteration.

    Returns: dict, with the old dictionary, with the
    """

    if len(old_dict) == 0:
        old_dict = dict(new_dict)
        old_dict['N_tp'] = [n_tp]
    else:
        for key in old_dict:
            if key == 'N_tp':
                old_dict[key].append(n_tp)
            else:
                for out_type in old_dict[key]:
                    old_dict[key][out_type] = np.vstack((old_dict[key][out_type], new_dict[key][out_type]))

    return old_dict
